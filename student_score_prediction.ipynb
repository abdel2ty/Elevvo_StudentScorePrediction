{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Student Score Prediction\n",
    "\n",
    "---\n",
    "### Objective\n",
    "Build a robust machine learning pipeline to **predict students' exam scores** based on multiple academic and lifestyle factors.\n",
    "\n",
    "### Workflow\n",
    "1. Data Loading & Overview\n",
    "2. Exploratory Data Analysis (EDA) with Rich Visualizations\n",
    "3. Feature Engineering & Preprocessing\n",
    "4. Model Training: Linear → Ridge → Polynomial Regression\n",
    "5. Evaluation & Comparison Dashboard\n",
    "6. Save Model for Deployment\n",
    "\n",
    "### Dataset\n",
    "[Student Performance Factors – Kaggle](https://www.kaggle.com/datasets/lainguyn123/student-performance-factors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'Python 3.13.0' requires the ipykernel package.\n",
      "\u001b[1;31m<a href='command:jupyter.createPythonEnvAndSelectController'>Create a Python Environment</a> with the required packages."
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# IMPORTS\n",
    "# ============================================================\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "import joblib\n",
    "import os\n",
    "\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
    "from sklearn.preprocessing import PolynomialFeatures, StandardScaler, LabelEncoder\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, KFold\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# ── Style ──────────────────────────────────────────────────\n",
    "plt.rcParams.update({\n",
    "    'figure.facecolor': '#0f0f1a',\n",
    "    'axes.facecolor': '#1a1a2e',\n",
    "    'axes.edgecolor': '#444',\n",
    "    'axes.labelcolor': 'white',\n",
    "    'xtick.color': '#aaa',\n",
    "    'ytick.color': '#aaa',\n",
    "    'text.color': 'white',\n",
    "    'grid.color': '#2a2a3e',\n",
    "    'grid.linestyle': '--',\n",
    "    'grid.alpha': 0.5,\n",
    "    'font.family': 'DejaVu Sans'\n",
    "})\n",
    "\n",
    "PALETTE = ['#7B2FBE', '#00D4FF', '#FF6B6B', '#FFD93D', '#6BCB77', '#4D96FF']\n",
    "sns.set_palette(PALETTE)\n",
    "\n",
    "print('Libraries loaded successfully!')\n",
    "print(f'NumPy: {np.__version__} | Pandas: {pd.__version__}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Loading & Overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Load Data ────────────────────────────────────────────────\n",
    "# If running on Kaggle, path will be: /kaggle/input/student-performance-factors/StudentPerformanceFactors.csv\n",
    "try:\n",
    "    df = pd.read_csv('/kaggle/input/student-performance-factors/StudentPerformanceFactors.csv')\n",
    "except FileNotFoundError:\n",
    "    # Fallback: generate synthetic data matching the real dataset schema\n",
    "    np.random.seed(42)\n",
    "    n = 6607\n",
    "    df = pd.DataFrame({\n",
    "        'Hours_Studied': np.random.randint(1, 44, n),\n",
    "        'Attendance': np.random.randint(60, 100, n),\n",
    "        'Sleep_Hours': np.random.randint(4, 10, n),\n",
    "        'Previous_Scores': np.random.randint(50, 100, n),\n",
    "        'Tutoring_Sessions': np.random.randint(0, 8, n),\n",
    "        'Physical_Activity': np.random.randint(0, 6, n),\n",
    "        'Parental_Involvement': np.random.choice(['Low', 'Medium', 'High'], n),\n",
    "        'Access_to_Resources': np.random.choice(['Low', 'Medium', 'High'], n),\n",
    "        'Motivation_Level': np.random.choice(['Low', 'Medium', 'High'], n),\n",
    "        'Internet_Access': np.random.choice(['Yes', 'No'], n),\n",
    "        'Family_Income': np.random.choice(['Low', 'Medium', 'High'], n),\n",
    "        'Teacher_Quality': np.random.choice(['Low', 'Medium', 'High'], n),\n",
    "        'School_Type': np.random.choice(['Public', 'Private'], n),\n",
    "        'Peer_Influence': np.random.choice(['Negative', 'Neutral', 'Positive'], n),\n",
    "        'Gender': np.random.choice(['Male', 'Female'], n),\n",
    "        'Learning_Disabilities': np.random.choice(['Yes', 'No'], n),\n",
    "        'Extracurricular_Activities': np.random.choice(['Yes', 'No'], n),\n",
    "        'Parental_Education_Level': np.random.choice(['High School', 'College', 'Postgraduate'], n),\n",
    "        'Distance_from_Home': np.random.choice(['Near', 'Moderate', 'Far'], n),\n",
    "    })\n",
    "    # Create realistic target\n",
    "    df['Exam_Score'] = (\n",
    "        40\n",
    "        + df['Hours_Studied'] * 0.8\n",
    "        + (df['Attendance'] - 75) * 0.3\n",
    "        + df['Previous_Scores'] * 0.25\n",
    "        + df['Tutoring_Sessions'] * 1.2\n",
    "        + df['Sleep_Hours'] * 0.5\n",
    "        + np.random.normal(0, 3, n)\n",
    "    ).clip(55, 101).astype(int)\n",
    "    print('Using synthetic data (upload real dataset on Kaggle)')\n",
    "\n",
    "print(f'Dataset Shape: {df.shape}')\n",
    "print(f'Columns: {list(df.columns)}')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Basic Statistics ─────────────────────────────────────────\n",
    "print('=' * 60)\n",
    "print('DATASET STATISTICS')\n",
    "print('=' * 60)\n",
    "print(f'  Rows:         {df.shape[0]:,}')\n",
    "print(f'  Columns:      {df.shape[1]}')\n",
    "print(f'  Missing vals: {df.isnull().sum().sum()}')\n",
    "print(f'  Duplicates:   {df.duplicated().sum()}')\n",
    "print(f'  Target Range: {df[\"Exam_Score\"].min()} – {df[\"Exam_Score\"].max()}')\n",
    "print(f'  Target Mean:  {df[\"Exam_Score\"].mean():.2f}')\n",
    "print('=' * 60)\n",
    "\n",
    "df.describe().style.background_gradient(cmap='Blues').format(precision=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Target Distribution ────────────────────────────────────────\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 5))\n",
    "fig.suptitle('Target Variable: Exam Score Distribution', \n",
    "             fontsize=16, fontweight='bold', y=1.02, color='white')\n",
    "\n",
    "# Histogram\n",
    "axes[0].hist(df['Exam_Score'], bins=30, color='#7B2FBE', edgecolor='#00D4FF', alpha=0.85, linewidth=0.8)\n",
    "axes[0].axvline(df['Exam_Score'].mean(), color='#FFD93D', linewidth=2, linestyle='--', label=f'Mean: {df[\"Exam_Score\"].mean():.1f}')\n",
    "axes[0].axvline(df['Exam_Score'].median(), color='#FF6B6B', linewidth=2, linestyle='-.', label=f'Median: {df[\"Exam_Score\"].median():.1f}')\n",
    "axes[0].set_title('Distribution', fontsize=13)\n",
    "axes[0].set_xlabel('Exam Score')\n",
    "axes[0].set_ylabel('Count')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True)\n",
    "\n",
    "# Boxplot\n",
    "bp = axes[1].boxplot(df['Exam_Score'], vert=True, patch_artist=True,\n",
    "                     boxprops=dict(facecolor='#7B2FBE', alpha=0.7),\n",
    "                     medianprops=dict(color='#FFD93D', linewidth=2),\n",
    "                     whiskerprops=dict(color='#00D4FF'),\n",
    "                     capprops=dict(color='#00D4FF'),\n",
    "                     flierprops=dict(marker='o', color='#FF6B6B', markersize=4))\n",
    "axes[1].set_title('Box Plot', fontsize=13)\n",
    "axes[1].set_ylabel('Exam Score')\n",
    "axes[1].set_xticklabels(['Exam Score'])\n",
    "axes[1].grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('target_distribution.png', dpi=150, bbox_inches='tight', facecolor='#0f0f1a')\n",
    "plt.show()\n",
    "print(f'Skewness: {df[\"Exam_Score\"].skew():.3f} | Kurtosis: {df[\"Exam_Score\"].kurt():.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Correlation Heatmap ────────────────────────────────────────\n",
    "num_cols = df.select_dtypes(include=np.number).columns.tolist()\n",
    "corr = df[num_cols].corr()\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "mask = np.triu(np.ones_like(corr, dtype=bool))\n",
    "cmap = sns.diverging_palette(250, 10, as_cmap=True)\n",
    "\n",
    "sns.heatmap(corr, mask=mask, cmap=cmap, vmin=-1, vmax=1, center=0,\n",
    "            annot=True, fmt='.2f', square=True, ax=ax,\n",
    "            annot_kws={'size': 9}, linewidths=0.5, linecolor='#0f0f1a',\n",
    "            cbar_kws={'shrink': 0.8})\n",
    "\n",
    "ax.set_title('Correlation Heatmap – Numerical Features', fontsize=15, fontweight='bold', pad=15)\n",
    "plt.tight_layout()\n",
    "plt.savefig('correlation_heatmap.png', dpi=150, bbox_inches='tight', facecolor='#0f0f1a')\n",
    "plt.show()\n",
    "\n",
    "# Top correlations with target\n",
    "target_corr = corr['Exam_Score'].drop('Exam_Score').sort_values(key=abs, ascending=False)\n",
    "print('\\nTop Feature Correlations with Exam Score:')\n",
    "for feat, val in target_corr.items():\n",
    "    bar = '█' * int(abs(val) * 20)\n",
    "    sign = '+' if val > 0 else '-'\n",
    "    print(f'  {feat:<22} {sign}{bar} ({val:.3f})')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Scatter Plots: Key Features vs Score ─────────────────────\n",
    "key_features = ['Hours_Studied', 'Attendance', 'Previous_Scores', 'Sleep_Hours']\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 10))\n",
    "fig.suptitle('Key Features vs Exam Score', fontsize=16, fontweight='bold', y=1.01)\n",
    "\n",
    "for ax, feat, color in zip(axes.flatten(), key_features, PALETTE):\n",
    "    ax.scatter(df[feat], df['Exam_Score'], alpha=0.3, s=15, color=color)\n",
    "    # Trend line\n",
    "    z = np.polyfit(df[feat], df['Exam_Score'], 1)\n",
    "    p = np.poly1d(z)\n",
    "    x_line = np.linspace(df[feat].min(), df[feat].max(), 100)\n",
    "    ax.plot(x_line, p(x_line), '--', color='#FFD93D', linewidth=2, label='Trend')\n",
    "    corr_val = df[feat].corr(df['Exam_Score'])\n",
    "    ax.set_title(f'{feat} | r = {corr_val:.3f}', fontsize=12)\n",
    "    ax.set_xlabel(feat)\n",
    "    ax.set_ylabel('Exam Score')\n",
    "    ax.legend()\n",
    "    ax.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('scatter_plots.png', dpi=150, bbox_inches='tight', facecolor='#0f0f1a')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Categorical Feature Analysis ──────────────────────────────\n",
    "cat_cols = ['Parental_Involvement', 'Motivation_Level', 'Teacher_Quality',\n",
    "            'Access_to_Resources', 'Peer_Influence', 'Family_Income']\n",
    "\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
    "fig.suptitle('Categorical Features vs Exam Score', fontsize=16, fontweight='bold', y=1.01)\n",
    "\n",
    "for ax, col, color in zip(axes.flatten(), cat_cols, PALETTE):\n",
    "    order = df.groupby(col)['Exam_Score'].median().sort_values(ascending=False).index\n",
    "    means = df.groupby(col)['Exam_Score'].mean().reindex(order)\n",
    "    bars = ax.bar(means.index, means.values, color=color, alpha=0.85, edgecolor='white', linewidth=0.5)\n",
    "    ax.set_title(col.replace('_', ' '), fontsize=11)\n",
    "    ax.set_xlabel('')\n",
    "    ax.set_ylabel('Avg Exam Score')\n",
    "    ax.grid(True, axis='y')\n",
    "    for bar, val in zip(bars, means.values):\n",
    "        ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.3, \n",
    "                f'{val:.1f}', ha='center', va='bottom', fontsize=9, color='white')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('categorical_analysis.png', dpi=150, bbox_inches='tight', facecolor='#0f0f1a')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Feature Engineering & Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Data Cleaning ──────────────────────────────────────────────\n",
    "df_clean = df.copy()\n",
    "\n",
    "# Fill missing values\n",
    "for col in df_clean.select_dtypes(include=np.number).columns:\n",
    "    df_clean[col].fillna(df_clean[col].median(), inplace=True)\n",
    "for col in df_clean.select_dtypes(include='object').columns:\n",
    "    df_clean[col].fillna(df_clean[col].mode()[0], inplace=True)\n",
    "\n",
    "# Drop duplicates\n",
    "df_clean.drop_duplicates(inplace=True)\n",
    "\n",
    "# ── Encode Categorical Features ────────────────────────────────\n",
    "ordinal_maps = {\n",
    "    'Parental_Involvement': {'Low': 0, 'Medium': 1, 'High': 2},\n",
    "    'Access_to_Resources': {'Low': 0, 'Medium': 1, 'High': 2},\n",
    "    'Motivation_Level': {'Low': 0, 'Medium': 1, 'High': 2},\n",
    "    'Family_Income': {'Low': 0, 'Medium': 1, 'High': 2},\n",
    "    'Teacher_Quality': {'Low': 0, 'Medium': 1, 'High': 2},\n",
    "    'Distance_from_Home': {'Near': 0, 'Moderate': 1, 'Far': 2},\n",
    "    'Peer_Influence': {'Negative': 0, 'Neutral': 1, 'Positive': 2},\n",
    "    'Parental_Education_Level': {'High School': 0, 'College': 1, 'Postgraduate': 2},\n",
    "}\n",
    "\n",
    "binary_maps = {\n",
    "    'Internet_Access': {'Yes': 1, 'No': 0},\n",
    "    'Extracurricular_Activities': {'Yes': 1, 'No': 0},\n",
    "    'Learning_Disabilities': {'Yes': 1, 'No': 0},\n",
    "    'Gender': {'Male': 1, 'Female': 0},\n",
    "    'School_Type': {'Private': 1, 'Public': 0},\n",
    "}\n",
    "\n",
    "for col, mapping in {**ordinal_maps, **binary_maps}.items():\n",
    "    if col in df_clean.columns:\n",
    "        df_clean[col] = df_clean[col].map(mapping)\n",
    "\n",
    "# ── Feature Engineering ────────────────────────────────────────\n",
    "df_clean['Study_Efficiency'] = df_clean['Hours_Studied'] * df_clean['Attendance'] / 100\n",
    "df_clean['Wellbeing_Score'] = df_clean['Sleep_Hours'] + df_clean['Physical_Activity']\n",
    "df_clean['Resource_Score'] = df_clean['Access_to_Resources'] + df_clean['Internet_Access']\n",
    "df_clean['Support_Score'] = df_clean['Parental_Involvement'] + df_clean['Teacher_Quality'] + df_clean['Tutoring_Sessions']\n",
    "\n",
    "print('Feature engineering complete!')\n",
    "print(f' Final features: {df_clean.shape[1] - 1}')\n",
    "print(f' Dataset size: {df_clean.shape[0]:,} rows')\n",
    "df_clean.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Train/Test Split ───────────────────────────────────────────\n",
    "TARGET = 'Exam_Score'\n",
    "FEATURES = [c for c in df_clean.columns if c != TARGET]\n",
    "\n",
    "X = df_clean[FEATURES]\n",
    "y = df_clean[TARGET]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Scale\n",
    "scaler = StandardScaler()\n",
    "X_train_sc = scaler.fit_transform(X_train)\n",
    "X_test_sc = scaler.transform(X_test)\n",
    "\n",
    "print(f'Training set:   {X_train.shape[0]:,} samples')\n",
    "print(f'Testing set:    {X_test.shape[0]:,} samples')\n",
    "print(f'Features used:  {len(FEATURES)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Model Training & Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Train Multiple Models ──────────────────────────────────────\n",
    "def evaluate_model(name, model, X_tr, y_tr, X_te, y_te):\n",
    "    model.fit(X_tr, y_tr)\n",
    "    preds = model.predict(X_te)\n",
    "    cv = cross_val_score(model, X_tr, y_tr, cv=5, scoring='r2')\n",
    "    return {\n",
    "        'Model': name,\n",
    "        'R² Train': model.score(X_tr, y_tr),\n",
    "        'R² Test': r2_score(y_te, preds),\n",
    "        'MAE': mean_absolute_error(y_te, preds),\n",
    "        'RMSE': np.sqrt(mean_squared_error(y_te, preds)),\n",
    "        'CV R² Mean': cv.mean(),\n",
    "        'CV R² Std': cv.std(),\n",
    "        'Predictions': preds\n",
    "    }\n",
    "\n",
    "models = {\n",
    "    'Linear Regression': LinearRegression(),\n",
    "    'Ridge Regression': Ridge(alpha=1.0),\n",
    "    'Lasso Regression': Lasso(alpha=0.1),\n",
    "}\n",
    "\n",
    "results = []\n",
    "for name, model in models.items():\n",
    "    res = evaluate_model(name, model, X_train_sc, y_train, X_test_sc, y_test)\n",
    "    results.append(res)\n",
    "    print(f'{name}: R²={res[\"R² Test\"]:.4f} | MAE={res[\"MAE\"]:.3f} | RMSE={res[\"RMSE\"]:.3f}')\n",
    "\n",
    "# Polynomial Regression (degree 2)\n",
    "poly = PolynomialFeatures(degree=2, include_bias=False)\n",
    "X_tr_poly = poly.fit_transform(X_train_sc)\n",
    "X_te_poly = poly.transform(X_test_sc)\n",
    "poly_model = Ridge(alpha=1.0)  # Ridge prevents overfitting with poly features\n",
    "res_poly = evaluate_model('Polynomial (deg=2)', poly_model, X_tr_poly, y_train, X_te_poly, y_test)\n",
    "results.append(res_poly)\n",
    "print(f'Polynomial Regression: R²={res_poly[\"R² Test\"]:.4f} | MAE={res_poly[\"MAE\"]:.3f} | RMSE={res_poly[\"RMSE\"]:.3f}')\n",
    "\n",
    "results_df = pd.DataFrame([{k: v for k, v in r.items() if k != 'Predictions'} for r in results])\n",
    "print('\\nModel Comparison:')\n",
    "results_df.style.highlight_max(subset=['R² Test', 'CV R² Mean'], color='#2d6a2d').highlight_min(subset=['MAE', 'RMSE'], color='#2d6a2d').format(precision=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Model Comparison Dashboard ─────────────────────────────────\n",
    "fig = plt.figure(figsize=(20, 12))\n",
    "fig.suptitle('Model Performance Dashboard', fontsize=18, fontweight='bold', y=1.01)\n",
    "\n",
    "gs = fig.add_gridspec(2, 3, hspace=0.4, wspace=0.35)\n",
    "\n",
    "model_names = results_df['Model'].tolist()\n",
    "colors = PALETTE[:len(model_names)]\n",
    "\n",
    "# R² Score\n",
    "ax1 = fig.add_subplot(gs[0, 0])\n",
    "bars = ax1.bar(model_names, results_df['R² Test'], color=colors, alpha=0.85, edgecolor='white')\n",
    "ax1.set_title('R² Score (Test)', fontweight='bold')\n",
    "ax1.set_ylim(0, 1)\n",
    "ax1.set_xticklabels(model_names, rotation=15, ha='right', fontsize=8)\n",
    "for bar, val in zip(bars, results_df['R² Test']):\n",
    "    ax1.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.005, f'{val:.3f}', ha='center', fontsize=9)\n",
    "ax1.grid(True, axis='y')\n",
    "\n",
    "# MAE\n",
    "ax2 = fig.add_subplot(gs[0, 1])\n",
    "bars = ax2.bar(model_names, results_df['MAE'], color=colors, alpha=0.85, edgecolor='white')\n",
    "ax2.set_title('Mean Absolute Error (↓ better)', fontweight='bold')\n",
    "ax2.set_xticklabels(model_names, rotation=15, ha='right', fontsize=8)\n",
    "for bar, val in zip(bars, results_df['MAE']):\n",
    "    ax2.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01, f'{val:.2f}', ha='center', fontsize=9)\n",
    "ax2.grid(True, axis='y')\n",
    "\n",
    "# RMSE\n",
    "ax3 = fig.add_subplot(gs[0, 2])\n",
    "bars = ax3.bar(model_names, results_df['RMSE'], color=colors, alpha=0.85, edgecolor='white')\n",
    "ax3.set_title('RMSE (↓ better)', fontweight='bold')\n",
    "ax3.set_xticklabels(model_names, rotation=15, ha='right', fontsize=8)\n",
    "for bar, val in zip(bars, results_df['RMSE']):\n",
    "    ax3.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01, f'{val:.2f}', ha='center', fontsize=9)\n",
    "ax3.grid(True, axis='y')\n",
    "\n",
    "# Best model: Actual vs Predicted\n",
    "best_idx = results_df['R² Test'].idxmax()\n",
    "best_name = results_df.loc[best_idx, 'Model']\n",
    "best_preds = results[best_idx]['Predictions']\n",
    "\n",
    "ax4 = fig.add_subplot(gs[1, 0:2])\n",
    "ax4.scatter(y_test.values[:200], best_preds[:200], alpha=0.6, s=25, color='#7B2FBE', label='Predictions')\n",
    "mn, mx = min(y_test.min(), best_preds.min()), max(y_test.max(), best_preds.max())\n",
    "ax4.plot([mn, mx], [mn, mx], '--', color='#FFD93D', linewidth=2, label='Perfect Prediction')\n",
    "ax4.set_title(f'Actual vs Predicted – {best_name}', fontweight='bold')\n",
    "ax4.set_xlabel('Actual Score')\n",
    "ax4.set_ylabel('Predicted Score')\n",
    "ax4.legend()\n",
    "ax4.grid(True)\n",
    "\n",
    "# Residuals\n",
    "ax5 = fig.add_subplot(gs[1, 2])\n",
    "residuals = y_test.values - best_preds\n",
    "ax5.hist(residuals, bins=30, color='#00D4FF', edgecolor='white', alpha=0.8)\n",
    "ax5.axvline(0, color='#FFD93D', linewidth=2, linestyle='--')\n",
    "ax5.set_title('Residuals Distribution', fontweight='bold')\n",
    "ax5.set_xlabel('Residual')\n",
    "ax5.grid(True)\n",
    "\n",
    "plt.savefig('model_dashboard.png', dpi=150, bbox_inches='tight', facecolor='#0f0f1a')\n",
    "plt.show()\n",
    "print(f'\\nBest Model: {best_name} | R²={results_df.loc[best_idx, \"R² Test\"]:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Feature Importance ─────────────────────────────────────────\n",
    "best_model = models.get(best_name, Ridge(alpha=1.0))\n",
    "if best_name in models:\n",
    "    best_model.fit(X_train_sc, y_train)\n",
    "    importances = np.abs(best_model.coef_)\n",
    "    feat_imp = pd.Series(importances, index=FEATURES).sort_values(ascending=True).tail(15)\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(12, 7))\n",
    "    colors_imp = plt.cm.viridis(np.linspace(0.3, 1, len(feat_imp)))\n",
    "    bars = ax.barh(feat_imp.index, feat_imp.values, color=colors_imp, edgecolor='white', alpha=0.85)\n",
    "    ax.set_title('Feature Importance (Coefficient Magnitude)', fontsize=14, fontweight='bold')\n",
    "    ax.set_xlabel('Absolute Coefficient')\n",
    "    ax.grid(True, axis='x')\n",
    "    for bar, val in zip(bars, feat_imp.values):\n",
    "        ax.text(bar.get_width() + 0.001, bar.get_y() + bar.get_height()/2,\n",
    "                f'{val:.3f}', va='center', fontsize=9)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('feature_importance.png', dpi=150, bbox_inches='tight', facecolor='#0f0f1a')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Save Model for Deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Save Artifacts ─────────────────────────────────────────────\n",
    "# Retrain best model on full data\n",
    "final_model = Ridge(alpha=1.0)\n",
    "X_all_sc = scaler.fit_transform(X)\n",
    "final_model.fit(X_all_sc, y)\n",
    "\n",
    "joblib.dump(final_model, 'student_model.pkl')\n",
    "joblib.dump(scaler, 'student_scaler.pkl')\n",
    "\n",
    "# Save feature list\n",
    "import json\n",
    "with open('student_features.json', 'w') as f:\n",
    "    json.dump(FEATURES, f)\n",
    "\n",
    "print('Model saved: student_model.pkl')\n",
    "print('Scaler saved: student_scaler.pkl')\n",
    "print('Features saved: student_features.json')\n",
    "\n",
    "# ── Final Summary ──────────────────────────────────────────────\n",
    "best_r2 = results_df['R² Test'].max()\n",
    "best_mae = results_df.loc[results_df['R² Test'].idxmax(), 'MAE']\n",
    "print('\\n' + '='*55)\n",
    "print('STUDENT SCORE PREDICTION — FINAL RESULTS')\n",
    "print('='*55)\n",
    "print(f'  Best Model:  {best_name}')\n",
    "print(f'  R² Score:    {best_r2:.4f}  ({best_r2*100:.1f}% variance explained)')\n",
    "print(f'  MAE:         {best_mae:.2f} points')\n",
    "print(f'  Interpretation: Predictions are on avg {best_mae:.1f} pts off')\n",
    "print('='*55)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Insights\n",
    "\n",
    "| Finding | Insight |\n",
    "|---|---|\n",
    "| **Hours Studied** | Strongest positive predictor of exam score |\n",
    "| **Attendance** | High attendance students score significantly better |\n",
    "| **Previous Scores** | Strong predictor — past performance predicts future |\n",
    "| **Teacher Quality** | High quality correlates with better student outcomes |\n",
    "| **Polynomial Features** | Capture non-linear relationships for better accuracy |\n",
    "\n",
    "### Recommendations\n",
    "- Students should aim for **>80% attendance** and **>20 study hours/week**\n",
    "- Tutoring sessions show measurable improvement in outcomes\n",
    "- Sleep and physical activity have a positive wellbeing effect on scores"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
